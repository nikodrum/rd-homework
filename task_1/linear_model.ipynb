{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"preprocessor\": [CountVectorizer(), TfidfVectorizer()],\n",
    "    \"preprocessor__ngram_range\": [(1, 1), (1, 2), (1, 3)],\n",
    "    \"preprocessor__min_df\": [2, 4, 7, 10],\n",
    "    \"model__estimator__C\": [0.01, 0.1, 0.5 , 1, 3, 5, 10, 25, 50],\n",
    "    \"preprocessor__stop_words\":['english'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"preprocessor\", CountVectorizer()),\n",
    "    (\"model\", OneVsRestClassifier(LogisticRegression(max_iter=1000)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = RandomizedSearchCV(pipe, grid, n_jobs=-1, cv=3, verbose=2, n_iter=20, scoring=\"f1_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  8.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('preprocessor',\n",
       "                                              CountVectorizer()),\n",
       "                                             ('model',\n",
       "                                              OneVsRestClassifier(estimator=LogisticRegression(max_iter=1000)))]),\n",
       "                   n_iter=20, n_jobs=-1,\n",
       "                   param_distributions={'model__estimator__C': [0.01, 0.1, 0.5,\n",
       "                                                                1, 3, 5, 10, 25,\n",
       "                                                                50],\n",
       "                                        'preprocessor': [CountVectorizer(),\n",
       "                                                         TfidfVectorizer(min_df=2,\n",
       "                                                                         ngram_range=(1,\n",
       "                                                                                      2),\n",
       "                                                                         stop_words='english')],\n",
       "                                        'preprocessor__min_df': [2, 4, 7, 10],\n",
       "                                        'preprocessor__ngram_range': [(1, 1),\n",
       "                                                                      (1, 2),\n",
       "                                                                      (1, 3)],\n",
       "                                        'preprocessor__stop_words': ['english']},\n",
       "                   scoring='f1_weighted', verbose=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(data_dict[\"data\"], data_dict[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 58.41825724, 105.97255095, 123.42507521,  43.40573096,\n",
       "        168.01206422, 216.63334346,  39.34012961,  93.22030536,\n",
       "         29.83187342,  14.22215327,  16.24928331,  36.78483693,\n",
       "         58.93230184,  23.10798796,  23.96666098,  25.72089267,\n",
       "         17.93893361,  24.18307964,  29.53789409,  65.33497938]),\n",
       " 'std_fit_time': array([0.61408216, 3.19579036, 4.05400163, 2.51145335, 6.218995  ,\n",
       "        1.21219044, 0.69316225, 4.77459331, 0.13816609, 0.40638519,\n",
       "        0.28180142, 0.55913289, 1.1185136 , 1.57392972, 0.39873733,\n",
       "        1.59727259, 0.4779861 , 0.50279934, 0.55727951, 1.06998362]),\n",
       " 'mean_score_time': array([4.93269388, 3.51377845, 3.94057409, 1.95364976, 4.06777541,\n",
       "        4.17593431, 5.15534107, 3.990364  , 5.03308829, 2.56051811,\n",
       "        3.88155071, 2.61512772, 3.88129814, 1.63543542, 1.56078005,\n",
       "        1.55365864, 3.55033573, 1.51701903, 2.31845681, 1.45193164]),\n",
       " 'std_score_time': array([0.07859608, 0.25142834, 0.45474147, 0.14180534, 0.18487355,\n",
       "        0.27210339, 0.50717355, 0.29289028, 0.3941516 , 0.23032075,\n",
       "        0.25198726, 0.07392478, 0.14787092, 0.02900289, 0.05722689,\n",
       "        0.07081589, 0.06488514, 0.10145873, 0.16692629, 0.1914791 ]),\n",
       " 'param_preprocessor__stop_words': masked_array(data=['english', 'english', 'english', 'english', 'english',\n",
       "                    'english', 'english', 'english', 'english', 'english',\n",
       "                    'english', 'english', 'english', 'english', 'english',\n",
       "                    'english', 'english', 'english', 'english', 'english'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_preprocessor__ngram_range': masked_array(data=[(1, 3), (1, 2), (1, 2), (1, 1), (1, 2), (1, 2), (1, 3),\n",
       "                    (1, 2), (1, 3), (1, 1), (1, 2), (1, 2), (1, 3), (1, 1),\n",
       "                    (1, 1), (1, 1), (1, 3), (1, 1), (1, 2), (1, 2)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_preprocessor__min_df': masked_array(data=[7, 2, 2, 4, 2, 2, 10, 2, 10, 4, 10, 10, 2, 10, 10, 4,\n",
       "                    10, 7, 4, 2],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_preprocessor': masked_array(data=[CountVectorizer(),\n",
       "                    TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "                    TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "                    CountVectorizer(),\n",
       "                    TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "                    CountVectorizer(), CountVectorizer(),\n",
       "                    TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "                    TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "                    TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "                    TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "                    CountVectorizer(),\n",
       "                    TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "                    CountVectorizer(), CountVectorizer(),\n",
       "                    CountVectorizer(),\n",
       "                    TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "                    CountVectorizer(),\n",
       "                    TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "                    CountVectorizer()],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_model__estimator__C': masked_array(data=[1, 5, 10, 25, 50, 3, 0.01, 3, 3, 1, 0.1, 5, 0.01, 3,\n",
       "                    10, 5, 0.5, 25, 10, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 3),\n",
       "   'preprocessor__min_df': 7,\n",
       "   'preprocessor': CountVectorizer(),\n",
       "   'model__estimator__C': 1},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 2),\n",
       "   'preprocessor__min_df': 2,\n",
       "   'preprocessor': TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "   'model__estimator__C': 5},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 2),\n",
       "   'preprocessor__min_df': 2,\n",
       "   'preprocessor': TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "   'model__estimator__C': 10},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 1),\n",
       "   'preprocessor__min_df': 4,\n",
       "   'preprocessor': CountVectorizer(),\n",
       "   'model__estimator__C': 25},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 2),\n",
       "   'preprocessor__min_df': 2,\n",
       "   'preprocessor': TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "   'model__estimator__C': 50},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 2),\n",
       "   'preprocessor__min_df': 2,\n",
       "   'preprocessor': CountVectorizer(),\n",
       "   'model__estimator__C': 3},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 3),\n",
       "   'preprocessor__min_df': 10,\n",
       "   'preprocessor': CountVectorizer(),\n",
       "   'model__estimator__C': 0.01},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 2),\n",
       "   'preprocessor__min_df': 2,\n",
       "   'preprocessor': TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "   'model__estimator__C': 3},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 3),\n",
       "   'preprocessor__min_df': 10,\n",
       "   'preprocessor': TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "   'model__estimator__C': 3},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 1),\n",
       "   'preprocessor__min_df': 4,\n",
       "   'preprocessor': TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "   'model__estimator__C': 1},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 2),\n",
       "   'preprocessor__min_df': 10,\n",
       "   'preprocessor': TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "   'model__estimator__C': 0.1},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 2),\n",
       "   'preprocessor__min_df': 10,\n",
       "   'preprocessor': CountVectorizer(),\n",
       "   'model__estimator__C': 5},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 3),\n",
       "   'preprocessor__min_df': 2,\n",
       "   'preprocessor': TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "   'model__estimator__C': 0.01},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 1),\n",
       "   'preprocessor__min_df': 10,\n",
       "   'preprocessor': CountVectorizer(),\n",
       "   'model__estimator__C': 3},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 1),\n",
       "   'preprocessor__min_df': 10,\n",
       "   'preprocessor': CountVectorizer(),\n",
       "   'model__estimator__C': 10},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 1),\n",
       "   'preprocessor__min_df': 4,\n",
       "   'preprocessor': CountVectorizer(),\n",
       "   'model__estimator__C': 5},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 3),\n",
       "   'preprocessor__min_df': 10,\n",
       "   'preprocessor': TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "   'model__estimator__C': 0.5},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 1),\n",
       "   'preprocessor__min_df': 7,\n",
       "   'preprocessor': CountVectorizer(),\n",
       "   'model__estimator__C': 25},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 2),\n",
       "   'preprocessor__min_df': 4,\n",
       "   'preprocessor': TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english'),\n",
       "   'model__estimator__C': 10},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 2),\n",
       "   'preprocessor__min_df': 2,\n",
       "   'preprocessor': CountVectorizer(),\n",
       "   'model__estimator__C': 0.1}],\n",
       " 'split0_test_score': array([0.87563845, 0.90536561, 0.90984171, 0.87382943, 0.91235447,\n",
       "        0.88800876, 0.85286021, 0.90072201, 0.87940915, 0.88055136,\n",
       "        0.79197706, 0.86844561, 0.58293034, 0.86686617, 0.86523802,\n",
       "        0.875854  , 0.84646604, 0.86785009, 0.90204128, 0.88937135]),\n",
       " 'split1_test_score': array([0.88714827, 0.90492105, 0.90714715, 0.88534453, 0.91280586,\n",
       "        0.8954046 , 0.85267572, 0.9041166 , 0.88778781, 0.88417428,\n",
       "        0.80237395, 0.8791013 , 0.58247118, 0.87774709, 0.87557266,\n",
       "        0.88618416, 0.8545206 , 0.87981734, 0.90515657, 0.89383923]),\n",
       " 'split2_test_score': array([0.87837389, 0.90569932, 0.90786744, 0.87602874, 0.91238063,\n",
       "        0.89197912, 0.84520694, 0.90298819, 0.88260483, 0.88250491,\n",
       "        0.80513559, 0.87450486, 0.59280437, 0.87328791, 0.87410262,\n",
       "        0.87704855, 0.8457545 , 0.87479471, 0.90374268, 0.89065382]),\n",
       " 'mean_test_score': array([0.88038687, 0.90532866, 0.90828543, 0.8784009 , 0.91251365,\n",
       "        0.89179749, 0.85024762, 0.90260893, 0.88326726, 0.88241018,\n",
       "        0.79982887, 0.87401726, 0.58606863, 0.87263373, 0.87163776,\n",
       "        0.87969557, 0.84891371, 0.87415405, 0.90364685, 0.89128813]),\n",
       " 'std_test_score': array([0.00490972, 0.0003188 , 0.00113906, 0.00499131, 0.0002069 ,\n",
       "        0.00302207, 0.0035651 , 0.00141155, 0.00345249, 0.00148057,\n",
       "        0.00566538, 0.00436381, 0.00476657, 0.00446614, 0.00456493,\n",
       "        0.00461397, 0.0039753 , 0.00490657, 0.00127362, 0.00187834]),\n",
       " 'rank_test_score': array([10,  3,  2, 12,  1,  6, 17,  5,  8,  9, 19, 14, 20, 15, 16, 11, 18,\n",
       "        13,  4,  7], dtype=int32)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 TfidfVectorizer(min_df=2, ngram_range=(1, 2),\n",
       "                                 stop_words='english')),\n",
       "                ('model',\n",
       "                 OneVsRestClassifier(estimator=LogisticRegression(C=50,\n",
       "                                                                  max_iter=1000)))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9125136533101775"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal-robot-dreams",
   "language": "python",
   "name": "personal-personal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
