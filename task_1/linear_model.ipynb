{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"preprocessor\": [CountVectorizer(), TfidfVectorizer()],\n",
    "    \"preprocessor__ngram_range\": [(1, 1), (1, 2), (1, 3)],\n",
    "    \"preprocessor__min_df\": [2, 4, 7, 10],\n",
    "    \"model__C\": [0.01, 0.1, 0.5 , 1, 3, 5, 10, 25, 50],\n",
    "    \"preprocessor__stop_words\":['english'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"preprocessor\", None),\n",
    "    (\"model\", LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = RandomizedSearchCV(pipe, grid, n_jobs=-1, cv=3, verbose=2, n_iter=10, scoring=\"f1_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('preprocessor', None),\n",
       "                                             ('model',\n",
       "                                              LogisticRegression(max_iter=1000))]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'model__C': [0.01, 0.1, 0.5, 1, 3, 5,\n",
       "                                                     10, 25, 50],\n",
       "                                        'preprocessor': [CountVectorizer(),\n",
       "                                                         TfidfVectorizer(min_df=4,\n",
       "                                                                         ngram_range=(1,\n",
       "                                                                                      3),\n",
       "                                                                         stop_words='english')],\n",
       "                                        'preprocessor__min_df': [2, 4, 7, 10],\n",
       "                                        'preprocessor__ngram_range': [(1, 1),\n",
       "                                                                      (1, 2),\n",
       "                                                                      (1, 3)],\n",
       "                                        'preprocessor__stop_words': ['english']},\n",
       "                   scoring='f1_weighted', verbose=2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(data_dict[\"data\"], data_dict[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  5.96001832,  60.71530191,  25.28752065,  30.95841153,\n",
       "        150.39543947, 117.96937895,  79.32046199,  53.82551336,\n",
       "        126.14912295,  71.51235572]),\n",
       " 'std_fit_time': array([ 0.07267525,  0.8672664 ,  1.7665695 ,  0.55693078,  7.80024554,\n",
       "         5.38572467, 20.03858451,  1.23473054,  3.61710803,  3.78695769]),\n",
       " 'mean_score_time': array([1.4634494 , 2.85911775, 4.34079003, 1.69937634, 4.26695768,\n",
       "        2.24433263, 3.39961871, 1.77313574, 1.87611564, 2.29447055]),\n",
       " 'std_score_time': array([0.03459573, 0.060789  , 0.17271255, 0.03371245, 0.27266669,\n",
       "        0.23308045, 0.61513404, 0.11885993, 0.07819127, 0.39035997]),\n",
       " 'param_preprocessor__stop_words': masked_array(data=['english', 'english', 'english', 'english', 'english',\n",
       "                    'english', 'english', 'english', 'english', 'english'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_preprocessor__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 3), (1, 1), (1, 3), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 3), (1, 2)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_preprocessor__min_df': masked_array(data=[7, 7, 7, 7, 7, 2, 7, 7, 4, 4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_preprocessor': masked_array(data=[TfidfVectorizer(min_df=4, ngram_range=(1, 3), stop_words='english'),\n",
       "                    TfidfVectorizer(min_df=4, ngram_range=(1, 3), stop_words='english'),\n",
       "                    TfidfVectorizer(min_df=4, ngram_range=(1, 3), stop_words='english'),\n",
       "                    TfidfVectorizer(min_df=4, ngram_range=(1, 3), stop_words='english'),\n",
       "                    CountVectorizer(), CountVectorizer(),\n",
       "                    CountVectorizer(), CountVectorizer(),\n",
       "                    TfidfVectorizer(min_df=4, ngram_range=(1, 3), stop_words='english'),\n",
       "                    TfidfVectorizer(min_df=4, ngram_range=(1, 3), stop_words='english')],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_model__C': masked_array(data=[0.1, 25, 0.1, 10, 25, 3, 0.1, 5, 50, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 1),\n",
       "   'preprocessor__min_df': 7,\n",
       "   'preprocessor': TfidfVectorizer(min_df=4, ngram_range=(1, 3), stop_words='english'),\n",
       "   'model__C': 0.1},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 2),\n",
       "   'preprocessor__min_df': 7,\n",
       "   'preprocessor': TfidfVectorizer(min_df=4, ngram_range=(1, 3), stop_words='english'),\n",
       "   'model__C': 25},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 3),\n",
       "   'preprocessor__min_df': 7,\n",
       "   'preprocessor': TfidfVectorizer(min_df=4, ngram_range=(1, 3), stop_words='english'),\n",
       "   'model__C': 0.1},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 1),\n",
       "   'preprocessor__min_df': 7,\n",
       "   'preprocessor': TfidfVectorizer(min_df=4, ngram_range=(1, 3), stop_words='english'),\n",
       "   'model__C': 10},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 3),\n",
       "   'preprocessor__min_df': 7,\n",
       "   'preprocessor': CountVectorizer(),\n",
       "   'model__C': 25},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 1),\n",
       "   'preprocessor__min_df': 2,\n",
       "   'preprocessor': CountVectorizer(),\n",
       "   'model__C': 3},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 2),\n",
       "   'preprocessor__min_df': 7,\n",
       "   'preprocessor': CountVectorizer(),\n",
       "   'model__C': 0.1},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 1),\n",
       "   'preprocessor__min_df': 7,\n",
       "   'preprocessor': CountVectorizer(),\n",
       "   'model__C': 5},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 3),\n",
       "   'preprocessor__min_df': 4,\n",
       "   'preprocessor': TfidfVectorizer(min_df=4, ngram_range=(1, 3), stop_words='english'),\n",
       "   'model__C': 50},\n",
       "  {'preprocessor__stop_words': 'english',\n",
       "   'preprocessor__ngram_range': (1, 2),\n",
       "   'preprocessor__min_df': 4,\n",
       "   'preprocessor': TfidfVectorizer(min_df=4, ngram_range=(1, 3), stop_words='english'),\n",
       "   'model__C': 5}],\n",
       " 'split0_test_score': array([0.8081726 , 0.89338407, 0.80098499, 0.89159826, 0.86436251,\n",
       "        0.87575211, 0.87180985, 0.86399155, 0.89900879, 0.89859412]),\n",
       " 'split1_test_score': array([0.81385928, 0.90173827, 0.80473217, 0.90253887, 0.88063085,\n",
       "        0.88450387, 0.88081096, 0.87505178, 0.90027639, 0.90282561]),\n",
       " 'split2_test_score': array([0.81044018, 0.89780342, 0.80965882, 0.8972    , 0.8712404 ,\n",
       "        0.87689029, 0.87211625, 0.87001229, 0.90266328, 0.90019493]),\n",
       " 'mean_test_score': array([0.81082402, 0.89764192, 0.80512533, 0.89711238, 0.87207792,\n",
       "        0.87904876, 0.87491235, 0.86968521, 0.90064948, 0.90053822]),\n",
       " 'std_test_score': array([0.00233739, 0.0034125 , 0.00355197, 0.00446691, 0.00666787,\n",
       "        0.00388523, 0.00417282, 0.00452124, 0.00151509, 0.00174447]),\n",
       " 'rank_test_score': array([ 9,  3, 10,  4,  7,  5,  6,  8,  1,  2], dtype=int32)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 TfidfVectorizer(min_df=4, ngram_range=(1, 3),\n",
       "                                 stop_words='english')),\n",
       "                ('model', LogisticRegression(C=50, max_iter=1000))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9006494839208106"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = fetch_20newsgroups(subset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 TfidfVectorizer(min_df=4, ngram_range=(1, 3),\n",
       "                                 stop_words='english')),\n",
       "                ('model', LogisticRegression(C=50, max_iter=1000))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = search.best_estimator_\n",
    "est.fit(data_dict['data'], data_dict['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.78      0.81       319\n",
      "           1       0.71      0.81      0.76       389\n",
      "           2       0.77      0.74      0.76       394\n",
      "           3       0.69      0.73      0.71       392\n",
      "           4       0.81      0.84      0.83       385\n",
      "           5       0.85      0.75      0.80       395\n",
      "           6       0.77      0.89      0.83       390\n",
      "           7       0.88      0.89      0.89       396\n",
      "           8       0.95      0.94      0.94       398\n",
      "           9       0.89      0.92      0.91       397\n",
      "          10       0.95      0.96      0.96       399\n",
      "          11       0.95      0.91      0.93       396\n",
      "          12       0.76      0.78      0.77       393\n",
      "          13       0.88      0.84      0.86       396\n",
      "          14       0.90      0.91      0.90       394\n",
      "          15       0.87      0.92      0.89       398\n",
      "          16       0.77      0.88      0.83       364\n",
      "          17       0.97      0.88      0.92       376\n",
      "          18       0.84      0.67      0.74       310\n",
      "          19       0.73      0.67      0.70       251\n",
      "\n",
      "    accuracy                           0.84      7532\n",
      "   macro avg       0.84      0.84      0.84      7532\n",
      "weighted avg       0.84      0.84      0.84      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "pred = est.predict(test['data'])\n",
    "print(metrics.classification_report(test['target'], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal-rd-course",
   "language": "python",
   "name": "personal-rd-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
